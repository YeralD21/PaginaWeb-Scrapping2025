#!/usr/bin/env python3
"""
Script independiente para scrapear solo noticias de El Comercio
y guardarlas en la base de datos.

Uso:
    python run_scraper_comercio.py [--limit N] [--categoria CATEGORIA]

Ejemplos:
    python run_scraper_comercio.py                    # Todas las noticias
    python run_scraper_comercio.py --limit 10         # Solo 10 noticias m√°s recientes
    python run_scraper_comercio.py --limit 10 --categoria deportes  # 10 de deportes

Este script:
- Usa Selenium para scrapear noticias de El Comercio (con fallback a BeautifulSoup)
- Guarda las noticias en la base de datos PostgreSQL
- Evita duplicados
- Prioriza noticias con im√°genes
- Muestra estad√≠sticas del proceso
"""

import sys
import os
import argparse
from datetime import datetime
import logging

# Agregar el directorio ra√≠z al path para importar m√≥dulos del backend
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('scraper_comercio.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

def main():
    """Funci√≥n principal para scrapear y guardar noticias de El Comercio"""
    # Configurar argumentos de l√≠nea de comandos
    parser = argparse.ArgumentParser(description='Scrapear noticias de El Comercio')
    parser.add_argument('--limit', type=int, default=None, help='N√∫mero m√°ximo de noticias a scrapear (por defecto: todas)')
    parser.add_argument('--categoria', type=str, default=None, help='Categor√≠a espec√≠fica (deportes, economia, mundo, politica, sociedad, tecnologia, cultura, espectaculos)')
    args = parser.parse_args()
    
    try:
        # Importar el scraper de El Comercio con Selenium
        try:
            from scraper_comercio_selenium import ScraperComercioSelenium
            scraper = ScraperComercioSelenium()
            logger.info("‚úÖ Usando ScraperComercio con Selenium")
        except ImportError as e:
            logger.error(f"‚ùå Error importando ScraperComercioSelenium: {e}")
            logger.error("Aseg√∫rate de que Selenium est√© instalado: pip install selenium")
            logger.error("Y que ChromeDriver est√© disponible en el PATH")
            sys.exit(1)
        
        # Importar el servicio de scraping para guardar en la base de datos
        # Asegurar que el path est√© correcto
        backend_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backend')
        if backend_path not in sys.path:
            sys.path.insert(0, backend_path)
        
        from scraping_service import ScrapingService
        
        logger.info("=" * 80)
        logger.info("üöÄ INICIANDO SCRAPING DE EL COMERCIO")
        if args.limit:
            logger.info(f"üìä L√≠mite: {args.limit} noticias")
        if args.categoria:
            logger.info(f"üìÇ Categor√≠a: {args.categoria}")
        logger.info("=" * 80)
        
        start_time = datetime.now()
        
        # Obtener noticias de El Comercio
        logger.info("üì∞ Extrayendo noticias de El Comercio...")
        
        if args.categoria:
            # Scrapear solo una categor√≠a espec√≠fica
            categoria_method = getattr(scraper, f'get_{args.categoria.lower()}', None)
            if categoria_method:
                all_news = categoria_method()
            else:
                logger.error(f"‚ùå Categor√≠a '{args.categoria}' no v√°lida")
                logger.info("Categor√≠as disponibles: deportes, economia, mundo, politica, sociedad, tecnologia, cultura, espectaculos")
                sys.exit(1)
        else:
            # Obtener noticias de todas las categor√≠as
            # Si hay l√≠mite, limitar por categor√≠a para obtener las m√°s recientes
            limit_per_category = max(3, args.limit // 5) if args.limit else 10
            all_news = scraper.get_all_news(limit_per_category=limit_per_category)
        
        # Aplicar l√≠mite final si se especific√≥
        if args.limit and len(all_news) > args.limit:
            # Priorizar noticias con im√°genes y m√°s recientes
            noticias_con_imagen = [n for n in all_news if n.get('imagen_url') and n.get('imagen_url').strip()]
            noticias_sin_imagen = [n for n in all_news if not (n.get('imagen_url') and n.get('imagen_url').strip())]
            
            # Ordenar por fecha de extracci√≥n (m√°s recientes primero)
            noticias_con_imagen.sort(key=lambda x: x.get('fecha_extraccion', ''), reverse=True)
            noticias_sin_imagen.sort(key=lambda x: x.get('fecha_extraccion', ''), reverse=True)
            
            # Tomar primero las que tienen imagen, luego las que no
            if len(noticias_con_imagen) >= args.limit:
                all_news = noticias_con_imagen[:args.limit]
            else:
                all_news = noticias_con_imagen + noticias_sin_imagen[:args.limit - len(noticias_con_imagen)]
            
            logger.info(f"üìä Limitando a {args.limit} noticias (priorizando con im√°genes y m√°s recientes)")
        
        logger.info(f"‚úÖ Total de noticias extra√≠das: {len(all_news)}")
        
        if not all_news:
            logger.warning("‚ö†Ô∏è No se extrajeron noticias. Verifica la conexi√≥n o la estructura del sitio.")
            return
        
        # Mostrar estad√≠sticas de extracci√≥n
        categorias = {}
        imagenes_con = 0
        imagenes_sin = 0
        
        for news in all_news:
            categoria = news.get('categoria', 'General')
            categorias[categoria] = categorias.get(categoria, 0) + 1
            
            if news.get('imagen_url') and news['imagen_url'].strip():
                imagenes_con += 1
            else:
                imagenes_sin += 1
        
        logger.info("\nüìä ESTAD√çSTICAS DE EXTRACCI√ìN:")
        logger.info(f"   Total de noticias: {len(all_news)}")
        logger.info(f"   Con imagen: {imagenes_con}")
        logger.info(f"   Sin imagen: {imagenes_sin}")
        logger.info(f"   Por categor√≠a:")
        for cat, count in sorted(categorias.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"      - {cat}: {count}")
        
        # Guardar en la base de datos
        logger.info("\nüíæ Guardando noticias en la base de datos...")
        scraping_service = ScrapingService()
        save_result = scraping_service.save_news_to_database_enhanced(all_news)
        
        # Mostrar resultados
        duration = (datetime.now() - start_time).total_seconds()
        
        logger.info("\n" + "=" * 80)
        logger.info("‚úÖ SCRAPING COMPLETADO")
        logger.info("=" * 80)
        logger.info(f"üìä Noticias extra√≠das: {len(all_news)}")
        logger.info(f"üíæ Noticias guardadas: {save_result['total_saved']}")
        logger.info(f"üîÑ Duplicados detectados: {save_result['duplicates_detected']}")
        logger.info(f"üö® Alertas activadas: {save_result['alerts_triggered']}")
        logger.info(f"‚è±Ô∏è  Duraci√≥n: {duration:.2f} segundos")
        
        if save_result.get('errors'):
            logger.warning(f"\n‚ö†Ô∏è Errores encontrados: {len(save_result['errors'])}")
            for error in save_result['errors'][:5]:  # Mostrar solo los primeros 5
                logger.warning(f"   - {error}")
        
        logger.info("\n" + "=" * 80)
        logger.info("‚ú® Las noticias ya est√°n disponibles en el frontend")
        logger.info("=" * 80)
        
    except ImportError as e:
        logger.error(f"‚ùå Error de importaci√≥n: {e}")
        logger.error("Aseg√∫rate de que todos los m√≥dulos est√©n disponibles.")
        logger.error("Ejecuta desde el directorio ra√≠z del proyecto.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error durante el scraping: {e}", exc_info=True)
        sys.exit(1)

if __name__ == "__main__":
    main()

