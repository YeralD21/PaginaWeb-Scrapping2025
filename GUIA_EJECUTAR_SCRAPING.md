# Gu√≠a: C√≥mo Ejecutar el Scraping desde el Backend

## üìã Resumen

Para que las noticias scrapeadas se **guarden autom√°ticamente en la base de datos** y aparezcan en tu frontend, necesitas ejecutar el scraping **desde el backend**, no directamente desde el script de Python.

---

## üöÄ Paso 1: Iniciar el Backend

### Opci√≥n A: Desde PowerShell (Windows)

```powershell
# Navegar al directorio del backend
cd backend

# Activar el entorno virtual (si lo tienes)
.\venv\Scripts\Activate.ps1

# Iniciar el servidor FastAPI
python main.py
```

### Opci√≥n B: Desde CMD (Windows)

```cmd
cd backend
venv\Scripts\activate
python main.py
```

### Opci√≥n C: Con uvicorn directamente

```powershell
cd backend
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**‚úÖ El backend deber√≠a iniciarse en:** `http://localhost:8000`

---

## üì° Paso 2: Ejecutar el Scraping

Una vez que el backend est√© corriendo, tienes varias opciones para ejecutar el scraping:

### **Opci√≥n 1: Usando Invoke-WebRequest (PowerShell) - RECOMENDADO**

```powershell
# Desde otra terminal PowerShell
Invoke-WebRequest -Uri http://localhost:8000/scraping/ejecutar -Method POST
```

**Para ver la respuesta en formato JSON:**
```powershell
$response = Invoke-WebRequest -Uri http://localhost:8000/scraping/ejecutar -Method POST
$response.Content | ConvertFrom-Json | ConvertTo-Json -Depth 10
```

### **Opci√≥n 2: Usando curl.exe (si est√° instalado)**

```powershell
# Usar curl.exe expl√≠citamente (no el alias de PowerShell)
curl.exe -X POST http://localhost:8000/scraping/ejecutar
```

### **Opci√≥n 3: Usando Postman**

1. Abre Postman
2. Crea una nueva petici√≥n
3. M√©todo: **POST**
4. URL: `http://localhost:8000/scraping/ejecutar`
5. Haz clic en **Send**

### **Opci√≥n 4: Desde el navegador (solo verificaci√≥n)**

Abre en tu navegador:
```
http://localhost:8000/docs
```

Busca el endpoint `/scraping/ejecutar` y haz clic en "Try it out" ‚Üí "Execute"

### **Opci√≥n 5: Script Python simple**

Crea un archivo `ejecutar_scraping.py`:

```python
import requests

response = requests.post("http://localhost:8000/scraping/ejecutar")
print("Status:", response.status_code)
print("Resultado:", response.json())
```

Ejecuta:
```powershell
python ejecutar_scraping.py
```

---

## üìä Respuesta del Endpoint

Cuando ejecutes el scraping, recibir√°s una respuesta JSON como esta:

```json
{
  "success": true,
  "total_extracted": 150,
  "total_saved": 120,
  "duplicates_detected": 30,
  "alerts_triggered": 5,
  "duration_seconds": 180
}
```

**Campos importantes:**
- `total_extracted`: Total de noticias scrapeadas
- `total_saved`: Noticias guardadas en la BD (despu√©s de filtrar duplicados)
- `duplicates_detected`: Noticias duplicadas que no se guardaron
- `duration_seconds`: Tiempo que tard√≥ el scraping

---

## ‚úÖ Paso 3: Verificar en el Frontend

Despu√©s de ejecutar el scraping:

1. **Abre tu frontend:** `http://localhost:3000`
2. **Ve a CNN:** `http://localhost:3000/diario/cnn-en-espa%C3%B1ol`
3. **Las noticias deber√≠an aparecer autom√°ticamente** desde la base de datos

---

## üîç Verificar Noticias en la Base de Datos

### Opci√≥n 1: Desde el Backend API

```powershell
# Ver todas las noticias de CNN
curl http://localhost:8000/noticias?diario=CNN%20en%20Espa%C3%B1ol
```

### Opci√≥n 2: Desde la documentaci√≥n interactiva

Abre: `http://localhost:8000/docs`

Busca el endpoint `GET /noticias` y prueba con:
- `diario`: `CNN en Espa√±ol`

---

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Ejecutar solo scraping de CNN

El endpoint `/scraping/ejecutar` ejecuta **TODOS los diarios** (Correo, Comercio, Popular, CNN).

Si quieres ejecutar solo CNN, puedes modificar temporalmente el c√≥digo o crear un endpoint espec√≠fico.

### Ver logs del scraping

Los logs aparecen en la terminal donde ejecutaste `python main.py`. Ver√°s:
- Progreso del scraping
- Noticias encontradas
- Errores (si los hay)

---

## üêõ Soluci√≥n de Problemas

### Error: "Connection refused"

**Problema:** El backend no est√° corriendo.

**Soluci√≥n:**
```powershell
cd backend
python main.py
```

### Error: "Module not found"

**Problema:** Faltan dependencias.

**Soluci√≥n:**
```powershell
cd backend
pip install -r requirements.txt
```

### Error: "Database connection failed"

**Problema:** PostgreSQL no est√° corriendo o la configuraci√≥n es incorrecta.

**Soluci√≥n:** Verifica tu archivo `.env` en el directorio `backend` y aseg√∫rate de que PostgreSQL est√© corriendo.

---

## üìù Resumen de Comandos R√°pidos

```powershell
# 1. Iniciar backend (Terminal 1)
cd backend
python main.py

# 2. Ejecutar scraping (Terminal 2)
curl -X POST http://localhost:8000/scraping/ejecutar

# 3. Ver noticias de CNN
curl http://localhost:8000/noticias?diario=CNN%20en%20Espa%C3%B1ol
```

---

## üéØ Flujo Completo

```
1. Backend corriendo (puerto 8000)
   ‚Üì
2. Ejecutar: POST /scraping/ejecutar
   ‚Üì
3. Scraping ejecuta todos los diarios (incluyendo CNN con Selenium)
   ‚Üì
4. Noticias se guardan en PostgreSQL
   ‚Üì
5. Frontend consulta la BD y muestra las noticias
   ‚Üì
6. ‚úÖ Noticias visibles en http://localhost:3000/diario/cnn-en-espa%C3%B1ol
```

---

## üí° Tips

- **El scraping puede tardar varios minutos** (especialmente CNN con Selenium)
- **No cierres la terminal** donde est√° corriendo el backend mientras se ejecuta el scraping
- **Los duplicados se detectan autom√°ticamente** y no se guardan
- **Puedes ejecutar el scraping m√∫ltiples veces** sin problemas (los duplicados se filtran)

