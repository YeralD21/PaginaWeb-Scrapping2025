# üìä INFORME COMPLETO: SCRAPING DE NOTICIAS DE REDES SOCIALES

## üìã √çndice
1. [Introducci√≥n](#introducci√≥n)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Fuentes de Datos](#fuentes-de-datos)
4. [Procedimiento de Scraping](#procedimiento-de-scraping)
5. [Uso de Selenium](#uso-de-selenium)
6. [Almacenamiento en Base de Datos](#almacenamiento-en-base-de-datos)
7. [Integraci√≥n Frontend](#integraci√≥n-frontend)
8. [Flujo Completo del Sistema](#flujo-completo-del-sistema)
9. [Configuraci√≥n y Activaci√≥n](#configuraci√≥n-y-activaci√≥n)
10. [Estructura de Datos](#estructura-de-datos)

---

## 1. Introducci√≥n

El sistema de scraping de redes sociales permite extraer noticias publicadas en plataformas como **Facebook**, **Twitter/X**, **Instagram** y **YouTube** de medios de comunicaci√≥n peruanos. El sistema utiliza **Selenium** para realizar scraping real cuando est√° activado, o genera datos mock para pruebas r√°pidas cuando Selenium est√° desactivado.

### Objetivo
Extraer noticias actualizadas de redes sociales de medios peruanos y almacenarlas en una base de datos PostgreSQL para su visualizaci√≥n en una interfaz web React.

---

## 2. Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    FRONTEND (React)                          ‚îÇ
‚îÇ  Componente: SocialMediaFeed.js                             ‚îÇ
‚îÇ  - Visualiza noticias filtradas por red social              ‚îÇ
‚îÇ  - Bot√≥n "Actualizar Noticias" ejecuta scraping             ‚îÇ
‚îÇ  - Filtros por plataforma (Twitter, Facebook, Instagram,    ‚îÇ
‚îÇ    YouTube)                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ HTTP POST
                            ‚îÇ /scraping/social-media/ejecutar
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BACKEND (FastAPI)                         ‚îÇ
‚îÇ  Archivo: backend/main.py                                   ‚îÇ
‚îÇ  Endpoint: /scraping/social-media/ejecutar                  ‚îÇ
‚îÇ  - Recibe petici√≥n de scraping                              ‚îÇ
‚îÇ  - Ejecuta ScrapingService.execute_social_scraping()        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              SCRAPING SERVICE                                ‚îÇ
‚îÇ  Archivo: backend/scraping_service.py                       ‚îÇ
‚îÇ  Clase: ScrapingService                                     ‚îÇ
‚îÇ  M√©todo: execute_social_scraping()                          ‚îÇ
‚îÇ  - Llama a MainScraper.scrape_social_media()                ‚îÇ
‚îÇ  - Guarda resultados en BD con save_news_to_database()      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              MAIN SCRAPER                                    ‚îÇ
‚îÇ  Archivo: scraping/main_scraper.py                          ‚îÇ
‚îÇ  Clase: MainScraper                                         ‚îÇ
‚îÇ  M√©todo: scrape_social_media()                              ‚îÇ
‚îÇ  - Selecciona scrapers seg√∫n USE_SELENIUM                   ‚îÇ
‚îÇ  - Ejecuta cada scraper de redes sociales                   ‚îÇ
‚îÇ  - Retorna lista unificada de noticias                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Scraper      ‚îÇ  ‚îÇ Scraper      ‚îÇ  ‚îÇ Scraper      ‚îÇ
‚îÇ Facebook     ‚îÇ  ‚îÇ Twitter      ‚îÇ  ‚îÇ Instagram    ‚îÇ
‚îÇ Selenium     ‚îÇ  ‚îÇ Selenium     ‚îÇ  ‚îÇ Selenium     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                   ‚îÇ                   ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              BASE DE DATOS (PostgreSQL)                      ‚îÇ
‚îÇ  Tablas:                                                     ‚îÇ
‚îÇ  - diarios: Almacena informaci√≥n de cada red social         ‚îÇ
‚îÇ  - noticias: Almacena todas las noticias extra√≠das          ‚îÇ
‚îÇ  - Campos: id, titulo, contenido, enlace, imagen_url,       ‚îÇ
‚îÇ    categoria, fecha_publicacion, diario_id, etc.            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3. Fuentes de Datos

### 3.1 Facebook
**Archivo**: `scraping/scraper_facebook_selenium.py`

**P√°ginas monitoreadas**:
- `elcomercio.pe` ‚Üí **El Comercio**
- `CorreoPeru` ‚Üí **Diario Correo**
- `cnn` ‚Üí **CNN en Espa√±ol**
- `elpopular.pe` ‚Üí **El Popular**
- `larepublicape` ‚Üí **La Rep√∫blica**

**URLs base**: `https://www.facebook.com/{pagina}`

### 3.2 Twitter/X
**Archivo**: `scraping/scraper_twitter_selenium.py`

**Cuentas monitoreadas**:
- `elcomercio_peru` ‚Üí @elcomercio_peru
- `DiarioCorreo` ‚Üí @DiarioCorreo
- `rppnoticias` ‚Üí @rppnoticias
- `Peru21` ‚Üí @Peru21
- `cnnespanol` ‚Üí @cnnespanol

**URLs base**: `https://twitter.com/{cuenta}`

### 3.3 Instagram
**Archivo**: `scraping/scraper_instagram_selenium.py`

**Cuentas monitoreadas**:
- `elcomercio.pe` ‚Üí @elcomercio.pe
- `diariocorreo` ‚Üí @diariocorreo
- `rppnoticias` ‚Üí @rppnoticias
- `cnnespanol` ‚Üí @cnnespanol

**URLs base**: `https://www.instagram.com/{cuenta}/`

**Nota**: Instagram requiere login, por lo que actualmente usa datos mock por defecto.

### 3.4 YouTube
**Archivo**: `scraping/scraper_youtube_selenium.py`

**Canales monitoreados** (usando IDs de canal verificados):
- `channel/UCyjzd3PHwG6TgCZCHHZWBYA` ‚Üí **El Comercio**
- `channel/UCuRsgsgZXkgjhHhbKEwJ1_A` ‚Üí **Diario Correo**
- `channel/UChOF38ucKKJm7BZqrB_55LA` ‚Üí **RPP Noticias**
- `channel/UC4vzdGCAYyE4DLKJZQC3cZQ` ‚Üí **Per√∫21**
- `channel/UCQi90C5nDOa5qe6OOmytdCA` ‚Üí **CNN en Espa√±ol**

**URLs base**: `https://www.youtube.com/{canal}/videos`

---

## 4. Procedimiento de Scraping

### 4.1 Flujo General

1. **Usuario hace clic en "Actualizar Noticias"** en el frontend
2. **Frontend env√≠a POST** a `/scraping/social-media/ejecutar`
3. **Backend ejecuta** `ScrapingService.execute_social_scraping()`
4. **MainScraper** determina qu√© scrapers usar (Selenium o Mock)
5. **Cada scraper** extrae noticias de su plataforma
6. **Noticias se guardan** en base de datos con detecci√≥n de duplicados
7. **Frontend recarga** y muestra las nuevas noticias

### 4.2 Proceso Detallado por Plataforma

#### 4.2.1 Facebook (Selenium)

**M√©todo**: `_scrape_page_real(fb_page, max_posts=5)`

1. **Configuraci√≥n del WebDriver**:
   ```python
   - Inicializa ChromeDriver
   - Configura opciones anti-detecci√≥n
   - User-Agent personalizado
   - Desactiva caracter√≠sticas de automatizaci√≥n
   ```

2. **Navegaci√≥n**:
   ```python
   - Abre URL: https://www.facebook.com/{pagina}
   - Espera hasta 20 segundos a que cargue contenido
   - Usa WebDriverWait con EC.presence_of_element_located
   - Selector principal: 'div[role="article"]'
   - Selector alternativo: '[data-pagelet]'
   ```

3. **Scroll para cargar m√°s contenido**:
   ```python
   - Ejecuta scrollTo(0, document.body.scrollHeight / 2)
   - Espera 2 segundos
   - Ejecuta scrollBy(0, 1000)
   - Espera 2 segundos m√°s
   ```

4. **Extracci√≥n de datos** (`_extract_post_data`):
   - **Texto del post**:
     - Busca `span[dir="auto"]` con texto > 20 caracteres
     - Alternativa: `div[data-testid]` con texto largo
   - **Imagen**:
     - Busca todas las `img` en el post
     - Filtra avatares, emojis y data URLs
     - Prefiere im√°genes con `alt` text (contenido)
   - **Enlace**:
     - Busca `a[href*="/posts/"]`
     - Si no existe, usa URL de la p√°gina
   - **Clasificaci√≥n de categor√≠a**:
     - Analiza palabras clave en el texto
     - Categor√≠as: Deportes, Econom√≠a, Pol√≠tica, Espect√°culos, Tecnolog√≠a, General

5. **Retorno de datos**:
   ```python
   {
       'titulo': texto[:200],
       'contenido': texto_completo,
       'enlace': url_del_post,
       'imagen_url': url_de_imagen,
       'categoria': categoria_detectada,
       'fecha_publicacion': datetime.now(timezone.utc),
       'fecha_extraccion': datetime.now(timezone.utc).isoformat(),
       'diario': 'Facebook',
       'diario_nombre': nombre_del_diario,
       'autor': nombre_del_diario
   }
   ```

#### 4.2.2 Twitter (Selenium)

**M√©todo**: `_scrape_account_real(account, max_tweets=10)`

1. **Navegaci√≥n**:
   - Abre `https://twitter.com/{cuenta}`
   - Espera elementos `article` o `[data-testid="tweet"]`
   - Realiza **scroll m√∫ltiple** (3 iteraciones) para cargar m√°s tweets recientes

2. **Extracci√≥n y filtros** (`_extract_tweet_data`):
   - **Detecci√≥n de tweets fijados**: `_is_pinned_tweet()` omite cualquier tweet marcado como "Tweet fijado" o "Pinned Tweet" para priorizar contenido actual.
   - **Fecha real**: `_extract_tweet_date()` intenta tres estrategias (atributo `datetime`, texto relativo como "hace 2 horas" y an√°lisis del HTML) para obtener la fecha exacta.
   - **Filtro de actualidad**: `_is_recent_tweet()` solo acepta publicaciones del **2025** o de los **√∫ltimos 30 d√≠as**. Tweets antiguos (ej. 2019, 2020) se descartan autom√°ticamente.
   - **Texto**:
     - Busca `div[lang]` o `[data-testid="tweetText"]`
     - Requiere m√≠nimo 20 caracteres para evitar tweets vac√≠os
   - **Imagen**:
     - Filtra im√°genes de perfil y emojis
     - Selecciona im√°genes de contenido cuando est√°n disponibles
   - **Enlace**:
     - Busca `a[href*="/status/"]`
     - Construye URL completa del tweet

3. **Clasificaci√≥n**:
   - Igual l√≥gica que Facebook (palabras clave por categor√≠a)
   - Registra logs informativos indicando si un tweet fue aceptado o filtrado por fecha

#### 4.2.3 YouTube (Selenium)

**M√©todo**: `_scrape_channel_real(channel, max_videos=5)`

1. **Navegaci√≥n**:
   - Abre `https://www.youtube.com/{canal}/videos`
   - Espera elementos `ytd-rich-item-renderer`
   - Scroll para cargar m√°s videos

2. **Extracci√≥n** (`_extract_video_data`):
   - **T√≠tulo**: `#video-title` (texto)
   - **Descripci√≥n**: `#metadata-line span` o fallback
   - **URL**: `href` de `#video-title`
   - **Thumbnail**: `src` de `img`, mejora calidad si es posible
   - **Clasificaci√≥n**: Basada en t√≠tulo y descripci√≥n

#### 4.2.4 Instagram (Mock por defecto)

**Nota**: Instagram requiere autenticaci√≥n, por lo que usa datos mock.

---

### 4.3 Modo Mock (Fallback)

Cuando Selenium no est√° activado o falla, cada scraper genera datos mock:

**Caracter√≠sticas**:
- T√≠tulos variados por categor√≠a
- Descripciones gen√©ricas
- Im√°genes de Picsum Photos con seeds √∫nicos
- Enlaces a perfiles oficiales (no posts espec√≠ficos)
- Fechas actuales

**Ejemplo Facebook Mock**:
```python
{
    'titulo': 'Pol√≠tica: Informaci√≥n pol√≠tica actual de √∫ltimo momento seg√∫n El Comercio',
    'contenido': 'Desde El Comercio: Informaci√≥n pol√≠tica actual...',
    'enlace': 'https://www.facebook.com/elcomercio.pe',
    'imagen_url': 'https://picsum.photos/800/400?random={seed}',
    'categoria': 'Pol√≠tica',
    ...
}
```

---

## 5. Uso de Selenium

### 5.1 Configuraci√≥n del WebDriver

Todos los scrapers Selenium comparten configuraci√≥n similar:

```python
chrome_options = Options()
# chrome_options.add_argument('--headless')  # Opcional: modo sin ventana
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--disable-blink-features=AutomationControlled')
chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
chrome_options.add_experimental_option('useAutomationExtension', False)
chrome_options.add_argument('user-agent=Mozilla/5.0...')

driver = webdriver.Chrome(options=chrome_options)
```

### 5.2 Esperas Expl√≠citas (WebDriverWait)

Para manejar contenido din√°mico:

```python
wait = WebDriverWait(driver, 20)  # Timeout de 20 segundos
wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[role="article"]')))
```

### 5.3 Manejo de Errores

- **Try/Except** alrededor de cada operaci√≥n
- **TimeoutException**: Si no carga el contenido en 20 segundos
- **NoSuchElementException**: Si no se encuentra un elemento espec√≠fico
- **Fallback a Mock**: Si falla completamente

### 5.4 Activaci√≥n de Selenium

**Variable de entorno**: `USE_SELENIUM`

**Valor**: `'true'` o `'True'` para activar

**Lugar de configuraci√≥n**:
- Al iniciar el backend: `$env:USE_SELENIUM="true"; python main.py` (Windows PowerShell)
- Script: `start_backend_selenium.bat`
- Archivo `.env`: `USE_SELENIUM=true`

**Verificaci√≥n en c√≥digo**:
```python
use_selenium = HAS_SELENIUM_SCRAPERS and os.getenv('USE_SELENIUM', 'False').lower() == 'true'
```

---

## 6. Almacenamiento en Base de Datos

### 6.1 Tabla `diarios`

**Campos principales**:
- `id`: Primary key
- `nombre`: Nombre √∫nico (ej: "Facebook", "Twitter", "Instagram", "YouTube")
- `url`: URL base de la plataforma
- `activo`: Boolean

**Valores para redes sociales**:
- Facebook: `nombre = 'Facebook'`
- Twitter: `nombre = 'Twitter'`
- Instagram: `nombre = 'Instagram'`
- YouTube: `nombre = 'YouTube'`

### 6.2 Tabla `noticias`

**Campos principales**:
- `id`: Primary key (auto-increment)
- `titulo`: T√≠tulo de la noticia (hasta 500 caracteres)
- `contenido`: Texto completo (TEXT)
- `enlace`: URL de la publicaci√≥n original
- `imagen_url`: URL de la imagen
- `categoria`: Categor√≠a (Pol√≠tica, Econom√≠a, Deportes, etc.)
- `fecha_publicacion`: Fecha de publicaci√≥n original
- `fecha_extraccion`: Fecha en que se extrajo la noticia
- `diario_id`: Foreign key a `diarios.id`
- `autor`: Autor o nombre del medio

**Campos extendidos** (para detecci√≥n de duplicados):
- `titulo_hash`: MD5 del t√≠tulo normalizado
- `contenido_hash`: MD5 del contenido normalizado
- `similarity_hash`: Hash para comparaci√≥n de similitud

**Campos geogr√°ficos**:
- `geographic_type`: Tipo (internacional, nacional, regional, local)
- `geographic_confidence`: Confianza de clasificaci√≥n (0-1)
- `geographic_keywords`: Palabras clave encontradas (JSON)

### 6.3 Proceso de Guardado

**Archivo**: `backend/scraping_service.py`
**M√©todo**: `save_news_to_database_enhanced()`

**Pasos**:

1. **Buscar el diario**:
   ```python
   diario = db.query(Diario).filter(Diario.nombre == news_item['diario']).first()
   ```
   - Busca por nombre exacto ("Facebook", "Twitter", etc.)

2. **Procesar fecha de publicaci√≥n**:
   ```python
   - Convierte a datetime si es string
   - Maneja diferentes formatos
   - Si falla, deja como None
   ```

3. **Detecci√≥n de duplicados**:
   ```python
   duplicate_check = self.duplicate_detector.check_duplicate(
       db=db,
       titulo=news_item['titulo'],
       contenido=news_item.get('contenido', ''),
       enlace=news_item.get('enlace', ''),
       diario_id=diario.id
   )
   ```
   - Verifica si ya existe una noticia similar
   - Compara t√≠tulo, contenido y enlace
   - Si es duplicado, **NO se guarda**

4. **Generaci√≥n de contenido** (si falta):
   ```python
   if not original_content or len(original_content) < 100:
       generated_content = generate_content_for_news(...)
       news_item['contenido'] = generated_content
   ```

5. **Clasificaci√≥n geogr√°fica**:
   ```python
   geographic_info = get_geographic_classification(
       title=news_item['titulo'],
       content=news_item.get('contenido', ''),
       category=news_item.get('categoria', '')
   )
   ```

6. **Crear registro**:
   ```python
   noticia = Noticia(
       titulo=enhanced_news['titulo'],
       contenido=enhanced_news.get('contenido', ''),
       enlace=enhanced_news.get('enlace', ''),
       imagen_url=enhanced_news.get('imagen_url', ''),
       categoria=enhanced_news['categoria'],
       fecha_publicacion=fecha_publicacion,
       fecha_extraccion=datetime.fromisoformat(enhanced_news['fecha_extraccion']),
       diario_id=diario.id,
       autor=enhanced_news.get('autor'),
       titulo_hash=enhanced_news.get('titulo_hash'),
       contenido_hash=enhanced_news.get('contenido_hash'),
       similarity_hash=enhanced_news.get('similarity_hash'),
       geographic_type=enhanced_news.get('geographic_type', 'nacional'),
       geographic_confidence=enhanced_news.get('geographic_confidence', 0.5),
       geographic_keywords=enhanced_news.get('geographic_keywords', {})
   )
   ```

7. **Guardar y verificar alertas**:
   ```python
   db.add(noticia)
   db.flush()  # Obtener ID
   alert_result = self.alert_system.process_news_alerts(db, noticia)
   db.commit()
   ```

### 6.4 ID de Noticia

El ID se genera autom√°ticamente por PostgreSQL:
- Tipo: `Integer` con `primary_key=True` y `index=True`
- Valor: Auto-incremental (1, 2, 3, ...)
- Uso: Identificador √∫nico para cada noticia en la base de datos

**C√≥mo se asigna**:
```python
db.add(noticia)
db.flush()  # Guarda en BD sin commit, pero obtiene el ID
noticia.id  # Ahora contiene el ID asignado
db.commit()  # Confirma el guardado
```

---

## 7. Integraci√≥n Frontend

### 7.1 Componente React

**Archivo**: `frontend/src/components/SocialMediaFeed.js`

**Funcionalidades**:
- Visualizaci√≥n de noticias filtradas por red social
- Contador de noticias por plataforma
- Bot√≥n "Actualizar Noticias" que ejecuta scraping
- Filtros individuales por plataforma (Twitter, Facebook, Instagram, YouTube)
- Cards visuales con imagen, t√≠tulo, categor√≠a y fecha

### 7.2 Endpoints Utilizados

#### GET `/social-media`
**Prop√≥sito**: Obtener noticias de redes sociales ya guardadas

**Par√°metros**:
- `categoria` (opcional): Filtrar por categor√≠a
- `diario` (opcional): Filtrar por plataforma (Twitter, Facebook, etc.)
- `limit`: L√≠mite de resultados (default: 100)
- `offset`: Desplazamiento para paginaci√≥n (default: 0)

**Respuesta**:
```json
[
  {
    "id": 123,
    "titulo": "T√≠tulo de la noticia",
    "contenido": "Contenido completo...",
    "enlace": "https://...",
    "imagen_url": "https://...",
    "categoria": "Pol√≠tica",
    "fecha_publicacion": "2025-01-15T10:30:00",
    "fecha_extraccion": "2025-01-15T11:00:00",
    "diario_id": 5,
    "diario_nombre": "Twitter"
  },
  ...
]
```

#### POST `/scraping/social-media/ejecutar`
**Prop√≥sito**: Ejecutar scraping de redes sociales

**Respuesta**:
```json
{
  "success": true,
  "total_extracted": 28,
  "total_saved": 9,
  "duplicates_detected": 19,
  "alerts_triggered": 0,
  "duration_seconds": 45,
  "error": null,
  "errors": []
}
```

### 7.3 Flujo en Frontend

1. **Carga inicial**:
   ```javascript
   useEffect(() => {
     fetchSocialNews();  // GET /social-media
     fetchAllNewsForCounts();  // Para contadores
   }, []);
   ```

2. **Actualizar noticias**:
   ```javascript
   const handleRefresh = async () => {
     setScraping(true);
     // POST /scraping/social-media/ejecutar
     const scrapResponse = await axios.post('http://localhost:8000/scraping/social-media/ejecutar');
     // Esperar 2 segundos
     setTimeout(() => {
       fetchSocialNews();  // Recargar noticias
       fetchAllNewsForCounts();  // Actualizar contadores
       setScraping(false);
     }, 2000);
   };
   ```

3. **Filtrado**:
   ```javascript
   const filteredNews = socialNews.filter(item => {
     if (activeFilter === 'all') return true;
     return item.diario_nombre === activeFilter;
   });
   ```

4. **Visualizaci√≥n**:
   - Cards con imagen, categor√≠a, t√≠tulo, fecha
   - Badge de plataforma (Twitter, Facebook, Instagram, YouTube)
   - Click en card abre enlace en nueva pesta√±a

---

## 8. Flujo Completo del Sistema

### Diagrama de Secuencia

```
Usuario (Frontend)          Backend API         ScrapingService    MainScraper    Scrapers Selenium    Base de Datos
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ  Click "Actualizar"       ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ  execute_social_    ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ  scraping()         ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ scrape_social_ ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ media()        ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ get_all_news()  ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ Setup WebDriver   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ<‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ Navegar a URL     ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ<‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ Extraer datos     ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ<‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                 ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ [lista noticias] ‚îÇ                 ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ save_news_to_  ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ database()     ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ INSERT
     ‚îÇ                           ‚îÇ                     ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
     ‚îÇ                           ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ {success: true, ...}      ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ GET /social-media         ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ SELECT * FROM     ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ noticias WHERE... ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
     ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ [lista noticias]          ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ                           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
     ‚îÇ Renderiza cards           ‚îÇ                     ‚îÇ                ‚îÇ                 ‚îÇ                   ‚îÇ
```

### Paso a Paso Detallado

1. **Usuario hace clic en "Actualizar Noticias"**
   - Frontend muestra indicador de carga
   - Se desactiva el bot√≥n temporalmente

2. **Frontend env√≠a POST a `/scraping/social-media/ejecutar`**
   - Usa `axios.post()` desde React
   - No requiere autenticaci√≥n (endpoint p√∫blico)

3. **Backend recibe la petici√≥n**
   - `main.py` ejecuta `ejecutar_social_scraping()`
   - Crea instancia de `ScrapingService`

4. **ScrapingService ejecuta scraping**
   - Llama a `self.main_scraper.scrape_social_media()`
   - Registra tiempo de inicio

5. **MainScraper selecciona scrapers**
   - Verifica `USE_SELENIUM` environment variable
   - Si `True`: Usa scrapers Selenium
   - Si `False`: Usa scrapers Mock

6. **Cada scraper ejecuta su l√≥gica**
   - **Selenium**: Abre navegador, navega, extrae datos
   - **Mock**: Genera datos de prueba

7. **Resultados se consolidan**
   - `MainScraper` retorna lista unificada de noticias
   - Formato est√°ndar para todas las plataformas

8. **Guardado en base de datos**
   - Para cada noticia:
     - Busca diario en BD
     - Verifica duplicados
     - Genera contenido si falta
     - Clasifica geogr√°ficamente
     - Crea registro `Noticia`
     - Guarda con commit

9. **Respuesta al frontend**
   - Retorna JSON con estad√≠sticas:
     - `total_extracted`: Noticias extra√≠das
     - `total_saved`: Noticias guardadas (sin duplicados)
     - `duplicates_detected`: Duplicados filtrados
     - `duration_seconds`: Tiempo total

10. **Frontend recarga noticias**
    - Espera 2 segundos
    - Hace GET a `/social-media`
    - Actualiza contadores
    - Renderiza nuevas noticias

---

## 9. Configuraci√≥n y Activaci√≥n

### 9.1 Activaci√≥n de Selenium

#### Opci√≥n 1: Script Batch (Windows)
**Archivo**: `start_backend_selenium.bat`
```batch
@echo off
cd backend
set USE_SELENIUM=true
python main.py
```

#### Opci√≥n 2: PowerShell
```powershell
cd backend
$env:USE_SELENIUM="true"
python main.py
```

#### Opci√≥n 3: Variables de entorno del sistema
- Windows: Panel de Control ‚Üí Sistema ‚Üí Variables de entorno
- Agregar: `USE_SELENIUM=true`

#### Opci√≥n 4: Archivo `.env` (no implementado actualmente)
```env
USE_SELENIUM=true
```

### 9.2 Verificaci√≥n de Activaci√≥n

**En logs del backend**:
```
üöÄ SELENIUM ACTIVADO - Usando scraping REAL de redes sociales
üöÄ Usando Selenium para scraping REAL de redes sociales
```

**Si no est√° activado**:
```
üì¶ Usando scrapers MOCK para redes sociales (configura USE_SELENIUM=true para usar Selenium)
```

### 9.3 Requisitos para Selenium

1. **Chrome instalado** en el sistema
2. **ChromeDriver** instalado y en PATH
3. **Selenium** instalado: `pip install selenium`
4. **Permisos** de escritura para logs temporales

---

## 10. Estructura de Datos

### 10.1 Formato de Noticia en Scrapers

Todos los scrapers retornan el mismo formato:

```python
{
    'titulo': str,              # T√≠tulo o texto principal (hasta 200 chars)
    'contenido': str,           # Contenido completo del post/tweet/video
    'enlace': str,              # URL de la publicaci√≥n original
    'imagen_url': str,          # URL de la imagen/thumbnail
    'categoria': str,           # Pol√≠tica, Econom√≠a, Deportes, etc.
    'fecha_publicacion': datetime,  # Fecha de publicaci√≥n (UTC)
    'fecha_extraccion': str,    # ISO format de fecha de extracci√≥n
    'diario': str,              # 'Facebook', 'Twitter', 'Instagram', 'YouTube'
    'diario_nombre': str,       # Nombre del medio (ej: 'El Comercio')
    'autor': str                # Autor o nombre del medio
}
```

### 10.2 Mapeo de Plataformas a Diarios

**Facebook**:
- `diario`: `'Facebook'`
- `diario_nombre`: `'El Comercio'`, `'Diario Correo'`, etc.

**Twitter**:
- `diario`: `'Twitter'`
- `diario_nombre`: `'Twitter'`
- `autor`: `'@elcomercio_peru'`, etc.

**Instagram**:
- `diario`: `'Instagram'`
- `diario_nombre`: `'Instagram'`
- `autor`: `'@elcomercio.pe'`, etc.

**YouTube**:
- `diario`: `'YouTube'`
- `diario_nombre`: `'El Comercio'`, `'Diario Correo'`, etc.
- `autor`: Nombre del canal

### 10.3 Clasificaci√≥n de Categor√≠as

**Palabras clave detectadas**:

- **Deportes**: `'deporte', 'futbol', 'selecci√≥n', 'gol', 'liga', 'atleta'`
- **Econom√≠a**: `'econ√≥m', 'd√≥lar', 'inflaci√≥n', 'mercado', 'negocio'`
- **Pol√≠tica**: `'pol√≠tic', 'congreso', 'presidente', 'gobierno'`
- **Espect√°culos**: `'actor', 'actriz', 'm√∫sica', 'pel√≠cula', 'celebrity', 'entretenimiento'`
- **Tecnolog√≠a**: `'tecnolog', 'digital', 'app', 'tech', 'smartphone'`
- **Internacional**: `'internacional', 'mundo', 'foreign', 'global'`
- **General**: Si no coincide con ninguna

---

## 11. Logs y Debugging

### 11.1 Logs del Backend

**Nivel**: `INFO` por defecto

**Ejemplos**:
```
üåê Iniciando scraping de redes sociales con Playwright...
üöÄ Usando Selenium para scraping REAL de redes sociales
üîç Scrapeando El Comercio...
üìÑ Accediendo a https://www.facebook.com/elcomercio.pe
‚úÖ Posts detectados usando CSS selector 'div[role="article"]'
üìä Encontrados 15 elementos de post
‚úÖ Post 1: Informaci√≥n pol√≠tica actual de √∫ltimo momento seg√∫n...
‚úÖ 3 posts reales obtenidos de El Comercio
üìä Total extra√≠do de scrapers: 28
‚úÖ Scraping de redes sociales completado: 9 noticias guardadas, 19 duplicados detectados
```

### 11.2 Logs del Frontend

**Consola del navegador** (F12 ‚Üí Console):
```javascript
Scraping completado: {success: true, total_extracted: 28, ...}
Error fetching social news: ...
```

### 11.3 Errores Comunes

1. **ChromeDriver no encontrado**:
   ```
   ‚ùå Error inicializando ChromeDriver: 'chromedriver' executable needs to be in PATH
   ```
   **Soluci√≥n**: Instalar ChromeDriver y agregarlo al PATH

2. **Timeout esperando contenido**:
   ```
   ‚è±Ô∏è Timeout esperando contenido de elcomercio.pe
   ```
   **Soluci√≥n**: La p√°gina tard√≥ m√°s de 20 segundos. Verificar conexi√≥n o aumentar timeout.

3. **Diario no encontrado en BD**:
   ```
   Diario no encontrado: Facebook
   ```
   **Soluci√≥n**: Ejecutar `init_diarios()` para crear registros de diarios.

---

## 12. Rendimiento y Optimizaciones

### 12.1 Tiempos Estimados

**Modo Mock**:
- Duraci√≥n total: **< 1 segundo**
- Por scraper: **~0.1 segundos**

**Modo Selenium**:
- Facebook: **~10-15 segundos** por p√°gina (5 p√°ginas = ~60-75s)
- Twitter: **~10-15 segundos** por cuenta (5 cuentas = ~60-75s)
- YouTube: **~10-15 segundos** por canal (5 canales = ~60-75s)
- Instagram: **~2 segundos** (mock)
- **Total**: **~3-4 minutos** para todas las plataformas

### 12.2 Optimizaciones Implementadas

1. **Filtro de actualidad en Twitter**: Solo guarda tweets recientes (2025 o √∫ltimos 30 d√≠as) y omite contenido fijado antiguo.
2. **Paralelizaci√≥n**: No implementada (secuencial)
3. **Headless mode**: Opcional (comentado en c√≥digo)
4. **Cache de im√°genes**: No implementado
5. **Detecci√≥n de duplicados**: Evita guardar noticias repetidas
6. **Paginaci√≥n en frontend**: L√≠mite de 100 noticias por defecto

### 12.3 Limitaciones

- **Rate limiting**: No implementado expl√≠citamente (pausas de 2-3 segundos entre requests)
- **Bloqueos**: Facebook/Twitter pueden bloquear si se hacen demasiadas peticiones
- **Cambios de HTML**: Selectores CSS pueden romperse si cambia la estructura

---

## 13. Conclusiones

El sistema de scraping de redes sociales es una soluci√≥n completa que:

1. ‚úÖ **Extrae noticias reales** usando Selenium cuando est√° activado
2. ‚úÖ **Genera datos mock** cuando Selenium no est√° disponible
3. ‚úÖ **Almacena en base de datos** con detecci√≥n de duplicados
4. ‚úÖ **Visualiza en interfaz web** con filtros por plataforma
5. ‚úÖ **Maneja errores** robustamente con fallbacks
6. ‚úÖ **Clasifica autom√°ticamente** por categor√≠a y geograf√≠a
7. ‚úÖ **Logs detallados** para debugging

**Mejoras futuras posibles**:
- Implementar autenticaci√≥n para Instagram
- Paralelizar scraping de m√∫ltiples plataformas
- Agregar m√°s plataformas (LinkedIn, TikTok)
- Implementar cache de resultados
- Agregar sistema de notificaciones push

---

**Fecha de creaci√≥n**: Enero 2025 (Actualizado octubre 2025 con filtros de fecha para Twitter)
**Versi√≥n**: 1.1
**Autor**: Sistema de Scraping de Noticias
