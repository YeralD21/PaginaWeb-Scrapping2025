# ðŸ¤– Sistema de AutomatizaciÃ³n Avanzado para Scraping de Diarios

## ðŸ“‹ DescripciÃ³n

Este sistema automatizado mejora significativamente tu scraping manual con las siguientes caracterÃ­sticas:

### âœ¨ CaracterÃ­sticas Principales

- **ðŸ•’ ProgramaciÃ³n por Horarios**: Ejecuta scraping en horarios especÃ­ficos (ej: 6:00, 9:00, 12:00, etc.)
- **ðŸ”„ Reintentos AutomÃ¡ticos**: Sistema robusto que reintenta en caso de fallos
- **ðŸš¨ DetecciÃ³n de Duplicados**: Evita guardar noticias repetidas
- **ðŸ“Š Monitoreo en Tiempo Real**: Visualiza el estado del sistema
- **ðŸ“ Logging Detallado**: Registros completos para debugging
- **âš¡ RecuperaciÃ³n AutomÃ¡tica**: Se recupera de errores sin intervenciÃ³n manual
- **ðŸ”§ ConfiguraciÃ³n Flexible**: Personaliza horarios y parÃ¡metros fÃ¡cilmente

---

## ðŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Requisitos Previos

AsegÃºrate de tener instalado:
- Python 3.8+
- Node.js 16+
- PostgreSQL (configurado)
- Dependencias del proyecto (`pip install -r requirements.txt`)

### 2. ConfiguraciÃ³n Inicial

```bash
# 1. Configurar el scheduler
python configure_scheduler.py

# 2. Verificar la configuraciÃ³n
python monitor_system.py
```

### 3. Archivos de ConfiguraciÃ³n

El sistema utiliza `scheduler_config.json` para la configuraciÃ³n:

```json
{
  "scraping_hours": [6, 9, 12, 15, 18, 21],  // Horarios de ejecuciÃ³n
  "max_retries": 3,                           // Reintentos mÃ¡ximos
  "retry_delay_minutes": 5,                   // Delay entre reintentos
  "log_level": "INFO",                        // Nivel de logging
  "enable_notifications": true                // Notificaciones
}
```

---

## ðŸŽ® Uso del Sistema

### OpciÃ³n 1: Sistema Completo (Recomendado)

```bash
# Inicia Backend + Scheduler + Frontend con monitoreo
python start_advanced_system.py
```

### OpciÃ³n 2: Solo Scheduler

```bash
# Solo el scheduler automÃ¡tico
cd scheduler
python advanced_scheduler.py
```

### OpciÃ³n 3: Monitoreo

```bash
# Ver estado actual
python monitor_system.py

# Monitoreo continuo cada 5 minutos
python monitor_system.py --continuous 5

# Guardar reporte
python monitor_system.py --save
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### ðŸ•’ Configurar Horarios

Usa el configurador interactivo:

```bash
python configure_scheduler.py
```

**Opciones de horarios predefinidos:**
- **Muy Frecuente**: Cada 2 horas (6:00, 8:00, 10:00, ...)
- **Frecuente**: Cada 3 horas (6:00, 9:00, 12:00, 15:00, 18:00, 21:00)
- **Moderado**: Cada 4 horas (6:00, 10:00, 14:00, 18:00, 22:00)
- **Espaciado**: Cada 6 horas (6:00, 12:00, 18:00)
- **Personalizado**: Define tus propios horarios

### ðŸ”§ ParÃ¡metros Importantes

| ParÃ¡metro | DescripciÃ³n | Valor Recomendado |
|-----------|-------------|-------------------|
| `scraping_hours` | Horas de ejecuciÃ³n (0-23) | `[6, 9, 12, 15, 18, 21]` |
| `max_retries` | Reintentos en caso de fallo | `3` |
| `retry_delay_minutes` | Minutos entre reintentos | `5` |
| `scraping_timeout` | Timeout en segundos | `600` (10 min) |
| `emergency_stop_after_failures` | Parar tras N fallos | `10` |

---

## ðŸ“Š Monitoreo y Logs

### ðŸ“ Archivos de Log

Los logs se guardan automÃ¡ticamente en:
- `logs/scheduler_YYYYMMDD.log` - Logs del scheduler
- `system_manager.log` - Logs del sistema
- `reports/system_report_*.json` - Reportes del sistema

### ðŸ” Verificar Estado

```bash
# Estado rÃ¡pido
python monitor_system.py

# InformaciÃ³n detallada
python monitor_system.py --save

# Monitoreo en tiempo real
python monitor_system.py --continuous 10
```

### ðŸ“ˆ Dashboard Web

Accede al dashboard en: `http://localhost:3000`

- Ver noticias en tiempo real
- EstadÃ­sticas de scraping
- Estado de los diarios
- Alertas y notificaciones

---

## ðŸ› ï¸ SoluciÃ³n de Problemas

### âŒ Problemas Comunes

#### 1. "Backend no disponible"
```bash
# Verificar que el backend estÃ© corriendo
curl http://localhost:8000

# Reiniciar si es necesario
cd backend
python -m uvicorn main:app --host 0.0.0.0 --port 8000
```

#### 2. "Base de datos offline"
```bash
# Verificar conexiÃ³n a PostgreSQL
python -c "from backend.database import test_connection; print(test_connection())"

# Verificar configuraciÃ³n en backend/.env
```

#### 3. "Scheduler no ejecuta"
```bash
# Verificar configuraciÃ³n
cat scheduler_config.json

# Ver logs del scheduler
tail -f logs/scheduler_*.log
```

#### 4. "Demasiados duplicados"
```bash
# Limpiar duplicados existentes
python backend/duplicate_detector.py --clean

# Verificar configuraciÃ³n de detecciÃ³n
```

### ðŸ”§ Comandos de DiagnÃ³stico

```bash
# Test completo del sistema
python -c "
from monitor_system import SystemMonitor
monitor = SystemMonitor()
report = monitor.generate_report()
monitor.print_status_summary(report)
"

# Verificar procesos activos
python -c "
import psutil
for p in psutil.process_iter(['pid', 'name', 'cmdline']):
    if 'uvicorn' in str(p.info['cmdline']) or 'scheduler' in str(p.info['cmdline']):
        print(f'PID {p.info[\"pid\"]}: {p.info[\"name\"]} - {p.info[\"cmdline\"]}')
"
```

---

## ðŸ“š Estructura del Sistema

```
ðŸ“ Proyecto/
â”œâ”€â”€ ðŸ¤– scheduler/
â”‚   â”œâ”€â”€ advanced_scheduler.py      # Scheduler principal
â”‚   â””â”€â”€ cron_job.py               # Scheduler bÃ¡sico (legacy)
â”œâ”€â”€ âš™ï¸  backend/
â”‚   â”œâ”€â”€ main.py                   # API FastAPI
â”‚   â”œâ”€â”€ scraping_service.py       # Servicio de scraping
â”‚   â””â”€â”€ models.py                 # Modelos de BD
â”œâ”€â”€ ðŸŒ frontend/
â”‚   â””â”€â”€ src/                      # AplicaciÃ³n React
â”œâ”€â”€ ðŸ“Š logs/                      # Logs del sistema
â”œâ”€â”€ ðŸ“‹ reports/                   # Reportes generados
â”œâ”€â”€ ðŸ”§ scheduler_config.json      # ConfiguraciÃ³n
â”œâ”€â”€ ðŸš€ start_advanced_system.py   # Iniciador del sistema
â”œâ”€â”€ ðŸ”§ configure_scheduler.py     # Configurador interactivo
â””â”€â”€ ðŸ“Š monitor_system.py          # Monitor del sistema
```

---

## ðŸ”„ Flujo de Trabajo Automatizado

```mermaid
graph TD
    A[Scheduler Iniciado] --> B{Hora Programada?}
    B -->|No| C[Esperar]
    C --> B
    B -->|SÃ­| D[Iniciar Scraping]
    D --> E[Scraper Diario Correo]
    D --> F[Scraper El Comercio]
    D --> G[Scraper El Popular]
    E --> H[Detectar Duplicados]
    F --> H
    G --> H
    H --> I[Guardar en BD]
    I --> J[Procesar Alertas]
    J --> K[Generar Logs]
    K --> L[Actualizar Frontend]
    L --> M[Esperar PrÃ³ximo Horario]
    M --> B
```

---

## ðŸ“ˆ MÃ©tricas y EstadÃ­sticas

El sistema rastrea automÃ¡ticamente:

- **ðŸ“° Noticias extraÃ­das por dÃ­a/hora**
- **ðŸ” Duplicados detectados**
- **â±ï¸ Tiempo de ejecuciÃ³n por scraping**
- **âŒ Errores y reintentos**
- **ðŸš¨ Alertas activadas**
- **ðŸ’¾ Uso de recursos del sistema**

### Ver EstadÃ­sticas

```bash
# EstadÃ­sticas rÃ¡pidas
curl http://localhost:8000/comparativa

# EstadÃ­sticas detalladas
curl http://localhost:8000/analytics/sentimientos

# Ãšltimas ejecuciones
python -c "
from backend.database import get_db
from backend.models import EstadisticaScraping
db = next(get_db())
stats = db.query(EstadisticaScraping).order_by(EstadisticaScraping.fecha_scraping.desc()).limit(5).all()
for s in stats:
    print(f'{s.fecha_scraping}: {s.cantidad_noticias} noticias en {s.duracion_segundos}s')
"
```

---

## ðŸŽ¯ Mejoras Implementadas vs Sistema Manual

| Aspecto | Sistema Manual | Sistema Automatizado |
|---------|----------------|----------------------|
| **EjecuciÃ³n** | Manual, cuando recuerdes | AutomÃ¡tica en horarios fijos |
| **Duplicados** | Posibles duplicados | DetecciÃ³n avanzada automÃ¡tica |
| **Errores** | Fallas detienen todo | Reintentos automÃ¡ticos |
| **Monitoreo** | Sin visibilidad | Dashboard y logs detallados |
| **ConfiguraciÃ³n** | CÃ³digo hardcodeado | ConfiguraciÃ³n flexible JSON |
| **RecuperaciÃ³n** | Manual | AutomÃ¡tica con fallbacks |
| **Notificaciones** | Ninguna | Alertas configurables |
| **EstadÃ­sticas** | BÃ¡sicas | MÃ©tricas completas |

---

## ðŸ†˜ Soporte y Mantenimiento

### Mantenimiento AutomÃ¡tico

El sistema incluye:
- âœ… Limpieza automÃ¡tica de logs antiguos
- âœ… RotaciÃ³n de archivos de log
- âœ… DetecciÃ³n de memoria y espacio en disco
- âœ… Parada de emergencia en caso de fallos crÃ­ticos

### Actualizaciones

```bash
# Actualizar configuraciÃ³n sin reiniciar
python configure_scheduler.py

# Reiniciar solo el scheduler
pkill -f "advanced_scheduler.py"
cd scheduler && python advanced_scheduler.py &

# Backup de la base de datos
pg_dump tu_base_datos > backup_$(date +%Y%m%d).sql
```

---

## ðŸŽ‰ Â¡Listo para Usar!

1. **Configura una vez**: `python configure_scheduler.py`
2. **Inicia el sistema**: `python start_advanced_system.py`
3. **Monitorea**: `python monitor_system.py --continuous 10`
4. **Disfruta**: Tu sistema ahora extrae noticias automÃ¡ticamente ðŸš€

### ðŸ“ž Â¿Necesitas Ayuda?

Si tienes problemas:
1. Revisa los logs en `logs/`
2. Ejecuta `python monitor_system.py` para diagnÃ³stico
3. Verifica la configuraciÃ³n en `scheduler_config.json`
4. Usa `python configure_scheduler.py` para reconfigurar

---

**Â¡Tu sistema de scraping ahora funciona 24/7 sin intervenciÃ³n manual!** ðŸŽ¯
